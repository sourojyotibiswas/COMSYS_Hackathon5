{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d7ab2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import collections\n",
    "\n",
    "# ================================\n",
    "# Custom Dataset Definition\n",
    "# ================================\n",
    "class FaceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        self.image_labels = []\n",
    "\n",
    "        persons = sorted(os.listdir(root_dir))\n",
    "        for idx, person in enumerate(persons):\n",
    "            self.class_to_idx[person] = idx\n",
    "            self.idx_to_class[idx] = person\n",
    "\n",
    "            person_folder = os.path.join(root_dir, person)\n",
    "            for root, _, files in os.walk(person_folder):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        self.samples.append((full_path, idx))\n",
    "                        self.image_labels.append((full_path, idx))  # for optional distortion-type analysis\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ================================\n",
    "# Evaluation Function\n",
    "# ================================\n",
    "def evaluate(model, loader, device, split_name):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Evaluating on {split_name} set\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"\\n {split_name} Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# ================================\n",
    "# Optional: Print Samples per Class\n",
    "# ================================\n",
    "def print_class_distribution(dataset):\n",
    "    counter = collections.Counter()\n",
    "    for _, label in dataset.samples:\n",
    "        counter[label] += 1\n",
    "    print(\"\\n Class distribution (Label Index â†’ Sample Count):\")\n",
    "    for k, v in sorted(counter.items()):\n",
    "        print(f\"Class {k}: {v} images\")\n",
    "\n",
    "# ================================\n",
    "# Main\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', type=str, default='/content/drive/MyDrive/Task_B')\n",
    "    parser.add_argument('--model_path', type=str, default='model_b.pth')\n",
    "    parser.add_argument('--img_size', type=int, default=224)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Transforms\n",
    "    eval_transforms = transforms.Compose([\n",
    "        transforms.Resize((args.img_size, args.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "    # Load Datasets\n",
    "    train_dataset = FaceDataset(os.path.join(args.data_path, \"train\"), transform=eval_transforms)\n",
    "    val_dataset = FaceDataset(os.path.join(args.data_path, \"val\"), transform=eval_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    num_classes = len(train_dataset.class_to_idx)\n",
    "\n",
    "    # Load Model\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optional: show how many samples per class\n",
    "    print_class_distribution(train_dataset)\n",
    "    print_class_distribution(val_dataset)\n",
    "\n",
    "    # Evaluate on both train and val sets\n",
    "    evaluate(model, train_loader, device, split_name=\"Train\")\n",
    "    evaluate(model, val_loader, device, split_name=\"Validation\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
